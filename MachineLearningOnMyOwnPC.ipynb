{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a single LRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_operator_diag(element_i, element_j):\n",
    "    a_i, bu_i = element_i\n",
    "    a_j, bu_j = element_j\n",
    "\n",
    "    return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "def init_lru_parameters(N, H, r_min = 0.0, r_max = 1, max_phase = 6.28):\n",
    "    # N: state dimension, H: model dimension\n",
    "    # Initialization of Lambda is complex valued distributed uniformly on ring\n",
    "    # between r_min and r_max, with phase in [0, max_phase].\n",
    "\n",
    "    u1 = np.random.uniform(size = (N,))\n",
    "    u2 = np.random.uniform(size = (N,))\n",
    "\n",
    "    nu_log = np.log(-0.5*np.log(u1*(r_max**2-r_min**2) + r_min**2))\n",
    "    theta_log = np.log(max_phase*u2)\n",
    "\n",
    "    # Glorot initialized Input/Output projection matrices\n",
    "    B_re = np.random.normal(size=(N,H))/np.sqrt(2*H)\n",
    "    B_im = np.random.normal(size=(N,H))/np.sqrt(2*H)\n",
    "    C_re = np.random.normal(size=(H,N))/np.sqrt(N)\n",
    "    C_im = np.random.normal(size=(H,N))/np.sqrt(N)\n",
    "    D = np.random.normal(size=(H,))\n",
    "\n",
    "    # Normalization\n",
    "    diag_lambda = np.exp(-np.exp(nu_log) + 1j*np.exp(theta_log))\n",
    "    gamma_log = np.log(np.sqrt(1-np.abs(diag_lambda)**2))\n",
    "\n",
    "    return nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log\n",
    "\n",
    "\n",
    "def forward_LRU(lru_parameters, input_sequence):\n",
    "    # Unpack the LRU parameters\n",
    "    nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = lru_parameters\n",
    "\n",
    "    # Initialize the hidden state\n",
    "    Lambda = jnp.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "    B_norm = (B_re + 1j*B_im) * jnp.expand_dims(jnp.exp(gamma_log), axis=-1)\n",
    "    C = C_re + 1j*C_im\n",
    "\n",
    "    Lambda_elements = jnp.repeat(Lambda[None, ...], input_sequence.shape[0], axis=0)\n",
    "\n",
    "    Bu_elements = jax.vmap(lambda u: B_norm @ u)(input_sequence)\n",
    "    elements = (Lambda_elements, Bu_elements)\n",
    "    _, inner_states = parallel_scan(binary_operator_diag, elements) # all x_k\n",
    "    y = jax.vmap(lambda x, u: (C @ x).real + D * u)(inner_states, input_sequence)\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "def loss_fn(lru_parameters, input_sequence, target_sequence):\n",
    "    y = forward_LRU(lru_parameters, input_sequence)\n",
    "    return jnp.mean((y - target_sequence)**2)\n",
    "\n",
    "\n",
    "def update(lru_parameters, input_sequence, target_sequence):\n",
    "    return  grad(loss_fn)(lru_parameters, input_sequence, target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_parameters(layers):\n",
    "    # Initialize the MLP parameters\n",
    "    parameters = []\n",
    "    for i in range(len(layers)-1):\n",
    "        W = np.random.normal(size=(layers[i], layers[i+1]))/np.sqrt(layers[i])\n",
    "        b = np.zeros((layers[i+1],))\n",
    "        parameters.append((W, b))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "@jit\n",
    "def forward_mlp(mlp_parameters, input, activation_function = jnp.tanh):\n",
    "    # Forward pass of the MLP\n",
    "    \n",
    "    x = input\n",
    "\n",
    "    for W, b in mlp_parameters:\n",
    "        x = x @ W + b\n",
    "        x = activation_function(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward_mlp_linear_layer(mlp_parameters, input, activation_function = jnp.tanh):\n",
    "    \n",
    "    x = input\n",
    "\n",
    "    # Only apply the MLP up to the second last layer\n",
    "    for W, b in mlp_parameters[:-1]:\n",
    "        x = x @ W + b\n",
    "        x = activation_function(x)\n",
    "\n",
    "    # Apply the last layer without activation function\n",
    "    W, b = mlp_parameters[-1]\n",
    "    x = x @ W + b\n",
    "\n",
    "    # Use the softmax function on the last layer\n",
    "    x = jax.nn.softmax(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(sequence_to_pool):\n",
    "    return jnp.max(sequence_to_pool, axis=0)\n",
    "\n",
    "def mean_pooling(sequence_to_pool):\n",
    "    return jnp.mean(sequence_to_pool, axis=0)\n",
    "\n",
    "def sum_pooling(sequence_to_pool):\n",
    "    return jnp.sum(sequence_to_pool, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "Linear_encoder_parameter  = init_mlp_parameters([1,3,5])\n",
    "seconday_parameters = init_mlp_parameters([5,5,5])\n",
    "LRU = init_lru_parameters(5, 5)\n",
    "Linear_decoder_parameter = init_mlp_parameters([5,3,2])\n",
    "\n",
    "model_parameters = [Linear_encoder_parameter, LRU, seconday_parameters, Linear_decoder_parameter]\n",
    "\n",
    "def model_forward(input_sequence, parameters):\n",
    "    Linear_encoder_parameter,  LRU, seconday_parameters, Linear_decoder_parameter = parameters\n",
    "\n",
    "    x = forward_mlp(Linear_encoder_parameter, input_sequence)\n",
    "    x = forward_LRU(LRU, x)\n",
    "    x = forward_mlp(seconday_parameters, x)\n",
    "    x = max_pooling(x)\n",
    "    x = forward_mlp_linear_layer(Linear_decoder_parameter, x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def model_loss(input_sequence, target_sequence, parameters):\n",
    "    y = model_forward(input_sequence, parameters)\n",
    "\n",
    "    # Binary cross entropy loss\n",
    "    return -jnp.mean(target_sequence * jnp.log(y) + (1-target_sequence) * jnp.log(1-y))\n",
    "\n",
    "@jit\n",
    "def model_grad(input_sequence, target_sequence, parameters):\n",
    "    return grad(model_loss, argnums=2)(input_sequence, target_sequence, parameters)\n",
    "\n",
    "@jit\n",
    "def parameter_update(parameters, gradients, learning_rate = 0.01):\n",
    "    new_parameters = []\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[0], gradients[0]):\n",
    "        im.append((parameter[0] - learning_rate * gradient[0], parameter[1] - learning_rate * gradient[1]))\n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[1], gradients[1]):\n",
    "        im.append(parameter - learning_rate * gradient)    \n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[2], gradients[2]):\n",
    "        im.append((parameter[0] - learning_rate * gradient[0], parameter[1] - learning_rate * gradient[1]))\n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[3], gradients[3]):\n",
    "        im.append((parameter[0] - learning_rate * gradient[0], parameter[1] - learning_rate * gradient[1]))\n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    return new_parameters\n",
    "\n",
    "\n",
    "# Test batch model forward\n",
    "batch_model_forward = vmap(model_forward, in_axes=(0, None))\n",
    "\n",
    "@jit\n",
    "def accuracy(input_sequences, target_sequences, parameters):\n",
    "    y = batch_model_forward(input_sequences, parameters)\n",
    "    return jnp.mean(jnp.argmax(y, axis=1) == jnp.argmax(target_sequences, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/waveforms_binary_classification.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m waveforms \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/waveforms_binary_classification.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mconcat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate\n\u001b[0;32m      6\u001b[0m sequences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcat((waveforms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaveform1\u001b[39m\u001b[38;5;124m\"\u001b[39m], waveforms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaveform2\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/waveforms_binary_classification.pkl'"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "waveforms = pkl.load(open(\"datasets/waveforms_binary_classification.pkl\", \"rb\"))\n",
    "\n",
    "np.concat = np.concatenate\n",
    "\n",
    "sequences = np.concat((waveforms[\"waveform1\"], waveforms[\"waveform2\"]))\n",
    "labels = np.concat((np.zeros(500),np.ones(500)))\n",
    "\n",
    "# Shuffle the dataset\n",
    "perm = np.random.permutation(1000)\n",
    "sequences = sequences[perm]\n",
    "labels = labels[perm]\n",
    "labels = one_hot(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4000, 1)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(sequences[:2].reshape((2, len(sequences[0]), 1)).shape)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    print(batch_model_forward(sequences[:2].reshape((2, len(sequences[0]), 1)), model_parameters).shape)\n",
    "except:\n",
    "    print(\"Model forward failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on 0: c:\\Users\\David\\Documents\\ML_jax\\.venv\\lib\\site-packages\\jax\\_src\\lax\\lax.py:3373: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "        x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 800/800 [100%] in 3.0s (267.80/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 1\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.4s (565.58/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 2\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.6s (491.08/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 3\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.4s (556.39/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 4\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.3s (601.66/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 5\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.3s (610.85/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 6\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.3s (607.71/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 7\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.3s (613.47/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 8\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.3s (609.20/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 9\n",
      "|████████████████████████████████████████| 800/800 [100%] in 1.3s (596.38/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batchsize = 1\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "train_sequences = sequences[:800].reshape((800, len(sequences[0]), 1))\n",
    "train_labels = labels[:800]\n",
    "\n",
    "test_sequences = sequences[800:].reshape((200, len(sequences[0]), 1))\n",
    "test_labels = labels[800:]\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    with alive_bar(800) as bar:\n",
    "        for x, y in zip(train_sequences, train_labels):\n",
    "            gradients = model_grad(x, jnp.array([y]), model_parameters)\n",
    "            model_parameters = parameter_update(model_parameters, gradients)\n",
    "            bar()\n",
    "\n",
    "    train_acc.append(accuracy(train_sequences, train_labels, model_parameters))\n",
    "    test_acc.append(accuracy(test_sequences, test_labels, model_parameters))\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc[i]}\")\n",
    "    print(f\"Test Accuracy: {test_acc[i]}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.00297597, 0.997024  ], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sequences[0].reshape((len(sequences[0]), 1))\n",
    "y = jnp.array(labels[0])\n",
    "\n",
    "value = model_grad(x, y, model_parameters)\n",
    "\n",
    "model_forward(x, model_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
