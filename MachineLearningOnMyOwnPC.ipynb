{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a single LRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_operator_diag(element_i, element_j):\n",
    "    a_i, bu_i = element_i\n",
    "    a_j, bu_j = element_j\n",
    "\n",
    "    return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "def init_lru_parameters(N, H, r_min = 0.0, r_max = 1, max_phase = 6.28):\n",
    "    # N: state dimension, H: model dimension\n",
    "    # Initialization of Lambda is complex valued distributed uniformly on ring\n",
    "    # between r_min and r_max, with phase in [0, max_phase].\n",
    "\n",
    "    u1 = np.random.uniform(size = (N,))\n",
    "    u2 = np.random.uniform(size = (N,))\n",
    "\n",
    "    nu_log = np.log(-0.5*np.log(u1*(r_max**2-r_min**2) + r_min**2))\n",
    "    theta_log = np.log(max_phase*u2)\n",
    "\n",
    "    # Glorot initialized Input/Output projection matrices\n",
    "    B_re = np.random.normal(size=(N,H))/np.sqrt(2*H)\n",
    "    B_im = np.random.normal(size=(N,H))/np.sqrt(2*H)\n",
    "    C_re = np.random.normal(size=(H,N))/np.sqrt(N)\n",
    "    C_im = np.random.normal(size=(H,N))/np.sqrt(N)\n",
    "    D = np.random.normal(size=(H,))\n",
    "\n",
    "    # Normalization\n",
    "    diag_lambda = np.exp(-np.exp(nu_log) + 1j*np.exp(theta_log))\n",
    "    gamma_log = np.log(np.sqrt(1-np.abs(diag_lambda)**2))\n",
    "\n",
    "    return nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log\n",
    "\n",
    "\n",
    "def forward_LRU(lru_parameters, input_sequence):\n",
    "    # Unpack the LRU parameters\n",
    "    nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = lru_parameters\n",
    "\n",
    "    # Initialize the hidden state\n",
    "    Lambda = jnp.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "    B_norm = (B_re + 1j*B_im) * jnp.expand_dims(jnp.exp(gamma_log), axis=-1)\n",
    "    C = C_re + 1j*C_im\n",
    "\n",
    "    Lambda_elements = jnp.repeat(Lambda[None, ...], input_sequence.shape[0], axis=0)\n",
    "\n",
    "    Bu_elements = jax.vmap(lambda u: B_norm @ u)(input_sequence)\n",
    "    elements = (Lambda_elements, Bu_elements)\n",
    "    _, inner_states = parallel_scan(binary_operator_diag, elements) # all x_k\n",
    "    y = jax.vmap(lambda x, u: (C @ x).real + D * u)(inner_states, input_sequence)\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "def loss_fn(lru_parameters, input_sequence, target_sequence):\n",
    "    y = forward_LRU(lru_parameters, input_sequence)\n",
    "    return jnp.mean((y - target_sequence)**2)\n",
    "\n",
    "\n",
    "def update(lru_parameters, input_sequence, target_sequence):\n",
    "    return  grad(loss_fn)(lru_parameters, input_sequence, target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_parameters(layers):\n",
    "    # Initialize the MLP parameters\n",
    "    parameters = []\n",
    "    for i in range(len(layers)-1):\n",
    "        W = np.random.normal(size=(layers[i], layers[i+1]))/np.sqrt(layers[i])\n",
    "        b = np.zeros((layers[i+1],))\n",
    "        parameters.append((W, b))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "@jit\n",
    "def forward_mlp(mlp_parameters, input, activation_function = jnp.tanh):\n",
    "    # Forward pass of the MLP\n",
    "    \n",
    "    x = input\n",
    "\n",
    "    for W, b in mlp_parameters:\n",
    "        x = x @ W + b\n",
    "        x = activation_function(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward_mlp_linear_layer(mlp_parameters, input, activation_function = jnp.tanh):\n",
    "    \n",
    "    x = input\n",
    "\n",
    "    # Only apply the MLP up to the second last layer\n",
    "    for W, b in mlp_parameters[:-1]:\n",
    "        x = x @ W + b\n",
    "        x = activation_function(x)\n",
    "\n",
    "    # Apply the last layer without activation function\n",
    "    W, b = mlp_parameters[-1]\n",
    "    x = x @ W + b\n",
    "\n",
    "    # Use the softmax function on the last layer\n",
    "    x = jax.nn.softmax(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(sequence_to_pool):\n",
    "    return jnp.max(sequence_to_pool, axis=0)\n",
    "\n",
    "def mean_pooling(sequence_to_pool):\n",
    "    return jnp.mean(sequence_to_pool, axis=0)\n",
    "\n",
    "def sum_pooling(sequence_to_pool):\n",
    "    return jnp.sum(sequence_to_pool, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "Linear_encoder_parameter  = init_mlp_parameters([1,3,5])\n",
    "seconday_parameters = init_mlp_parameters([5,5,5])\n",
    "LRU = init_lru_parameters(5, 5)\n",
    "Linear_decoder_parameter = init_mlp_parameters([5,3,2])\n",
    "\n",
    "model_parameters = (Linear_encoder_parameter, seconday_parameters, LRU, Linear_decoder_parameter)\n",
    "\n",
    "def model_forward(input_sequence, parameters):\n",
    "    Linear_encoder_parameter, seconday_parameters, LRU, Linear_decoder_parameter = parameters\n",
    "\n",
    "    x = forward_mlp(Linear_encoder_parameter, input_sequence)\n",
    "    x = forward_LRU(LRU, x)\n",
    "    x = forward_mlp(seconday_parameters, x)\n",
    "    x = max_pooling(x)\n",
    "    x = forward_mlp_linear_layer(Linear_decoder_parameter, x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def model_loss(input_sequence, target_sequence, parameters):\n",
    "    y = model_forward(input_sequence, parameters)\n",
    "\n",
    "    # Binary cross entropy loss\n",
    "    return -jnp.mean(target_sequence * jnp.log(y) + (1-target_sequence) * jnp.log(1-y))\n",
    "\n",
    "def model_grad(input_sequence, target_sequence, parameters):\n",
    "    return grad(model_loss)(input_sequence, target_sequence, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.59122086, 0.4087791 ], dtype=float32)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forward(jnp.array([[1.0], [2.0], [3.0]]), model_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "waveforms = pkl.load(open(\"waveforms.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "sequences = np.concat((waveforms[\"waveform1\"], waveforms[\"waveform2\"]))\n",
    "labels = np.concat((np.zeros(500),np.ones(500)))\n",
    "\n",
    "# Shuffle the dataset\n",
    "perm = np.random.permutation(1000)\n",
    "sequences = sequences[perm]\n",
    "labels = labels[perm]\n",
    "labels = one_hot(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\Documents\\ML_jax\\.venv\\lib\\site-packages\\jax\\_src\\lax\\lax.py:3373: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[[-5.8554634e-03],\n",
       "         [ 2.5951770e-06],\n",
       "         [-4.3208975e-06]]],\n",
       "\n",
       "\n",
       "       [[[ 2.5951726e-06],\n",
       "         [-4.0689006e-04],\n",
       "         [-2.4504808e-07]]],\n",
       "\n",
       "\n",
       "       [[[-4.3208984e-06],\n",
       "         [-2.4504811e-07],\n",
       "         [-5.2616673e-05]]]], dtype=float32)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacobian(jax.jacobian(model_loss))(jnp.array([[1.0], [2.0], [3.0]]), 1, model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'jaxlib.xla_extension.ArrayImpl' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[320], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sequences, labels):\n\u001b[1;32m----> 7\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m#model_parameters = [p - 0.01*g for p, g in zip(model_parameters, grad)]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[313], line 31\u001b[0m, in \u001b[0;36mmodel_grad\u001b[1;34m(input_sequence, target_sequence, parameters)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_grad\u001b[39m(input_sequence, target_sequence, parameters):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_loss\u001b[49m\u001b[43m)\u001b[49m(input_sequence, target_sequence, parameters)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'jaxlib.xla_extension.ArrayImpl' object is not callable"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batchsize = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    for x, y in zip(sequences, labels):\n",
    "        grad = model_grad(x.reshape((len(x), 1)), y, model_parameters)\n",
    "        #model_parameters = [p - 0.01*g for p, g in zip(model_parameters, grad)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([[1.0], [2.0], [3.0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "Gradient: [2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "# Define a scalar-valued function\n",
    "def my_function(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Use grad on the function\n",
    "grad_fn = jax.grad(my_function)\n",
    "\n",
    "# Evaluate gradient at a specific point\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "gradient = grad_fn(x)\n",
    "print(\"Gradient:\", gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
