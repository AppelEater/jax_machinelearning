{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataloader generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='datasets/dataset_freq_100_SNR_0.pkl'>\n"
     ]
    }
   ],
   "source": [
    "SNR = jnp.arange(0,20, 5)\n",
    "frequencies = jnp.array([100, 200, 300, 400, 500])\n",
    "\n",
    "print(open(\"datasets/dataset_freq_100_SNR_0.pkl\", \"rb\"))\n",
    "\n",
    "def dataloader(batch_size):\n",
    "    while True:\n",
    "        for snr, freq in itertools.product(SNR, frequencies):\n",
    "            with open(f\"datasets/dataset_freq_{freq}_SNR_{snr}.pkl\", \"rb\") as f:\n",
    "                data = pkl.load(f)\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                yield data[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/dataset_freq_100_freq_SNR_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dataloader(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mdataloader\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m snr, freq \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(SNR, frequencies):\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/dataset_freq_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfreq\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_freq_SNR_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msnr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m             data \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data), batch_size):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/dataset_freq_100_freq_SNR_0.pkl'"
     ]
    }
   ],
   "source": [
    "for x in dataloader(10):\n",
    "    print(x)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a single LRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_operator_diag(element_i, element_j):\n",
    "    a_i, bu_i = element_i\n",
    "    a_j, bu_j = element_j\n",
    "\n",
    "    return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "def init_lru_parameters(N, H, r_min = 0.0, r_max = 1, max_phase = 6.28):\n",
    "    # N: state dimension, H: model dimension\n",
    "    # Initialization of Lambda is complex valued distributed uniformly on ring\n",
    "    # between r_min and r_max, with phase in [0, max_phase].\n",
    "\n",
    "    u1 = np.random.uniform(size = (N,))\n",
    "    u2 = np.random.uniform(size = (N,))\n",
    "\n",
    "    nu_log = np.log(-0.5*np.log(u1*(r_max**2-r_min**2) + r_min**2))\n",
    "    theta_log = np.log(max_phase*u2)\n",
    "\n",
    "    # Glorot initialized Input/Output projection matrices\n",
    "    B_re = np.random.normal(size=(N,H))/np.sqrt(2*H)\n",
    "    B_im = np.random.normal(size=(N,H))/np.sqrt(2*H)\n",
    "    C_re = np.random.normal(size=(H,N))/np.sqrt(N)\n",
    "    C_im = np.random.normal(size=(H,N))/np.sqrt(N)\n",
    "    D = np.random.normal(size=(H,))\n",
    "\n",
    "    # Normalization\n",
    "    diag_lambda = np.exp(-np.exp(nu_log) + 1j*np.exp(theta_log))\n",
    "    gamma_log = np.log(np.sqrt(1-np.abs(diag_lambda)**2))\n",
    "\n",
    "    return nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log\n",
    "\n",
    "\n",
    "def forward_LRU(lru_parameters, input_sequence):\n",
    "    # Unpack the LRU parameters\n",
    "    nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = lru_parameters\n",
    "\n",
    "    # Initialize the hidden state\n",
    "    Lambda = jnp.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "    B_norm = (B_re + 1j*B_im) * jnp.expand_dims(jnp.exp(gamma_log), axis=-1)\n",
    "    C = C_re + 1j*C_im\n",
    "\n",
    "    Lambda_elements = jnp.repeat(Lambda[None, ...], input_sequence.shape[0], axis=0)\n",
    "\n",
    "    Bu_elements = jax.vmap(lambda u: B_norm @ u)(input_sequence)\n",
    "    elements = (Lambda_elements, Bu_elements)\n",
    "    _, inner_states = parallel_scan(binary_operator_diag, elements) # all x_k\n",
    "    y = jax.vmap(lambda x, u: (C @ x).real + D * u)(inner_states, input_sequence)\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "def loss_fn(lru_parameters, input_sequence, target_sequence):\n",
    "    y = forward_LRU(lru_parameters, input_sequence)\n",
    "    return jnp.mean((y - target_sequence)**2)\n",
    "\n",
    "\n",
    "def update(lru_parameters, input_sequence, target_sequence):\n",
    "    return  grad(loss_fn)(lru_parameters, input_sequence, target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_parameters(layers):\n",
    "    # Initialize the MLP parameters\n",
    "    parameters = []\n",
    "    for i in range(len(layers)-1):\n",
    "        W = np.random.normal(size=(layers[i], layers[i+1]))/np.sqrt(layers[i])\n",
    "        b = np.zeros((layers[i+1],))\n",
    "        parameters.append((W, b))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "@jit\n",
    "def forward_mlp(mlp_parameters, input, activation_function = jnp.tanh):\n",
    "    # Forward pass of the MLP\n",
    "    \n",
    "    x = input\n",
    "\n",
    "    for W, b in mlp_parameters:\n",
    "        x = x @ W + b\n",
    "        x = activation_function(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward_mlp_linear_layer(mlp_parameters, input, activation_function = jnp.tanh):\n",
    "    \n",
    "    x = input\n",
    "\n",
    "    # Only apply the MLP up to the second last layer\n",
    "    for W, b in mlp_parameters[:-1]:\n",
    "        x = x @ W + b\n",
    "        x = activation_function(x)\n",
    "\n",
    "    # Apply the last layer without activation function\n",
    "    W, b = mlp_parameters[-1]\n",
    "    x = x @ W + b\n",
    "\n",
    "    # Use the softmax function on the last layer\n",
    "    x = jax.nn.softmax(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(sequence_to_pool):\n",
    "    return jnp.max(sequence_to_pool, axis=0)\n",
    "\n",
    "def mean_pooling(sequence_to_pool):\n",
    "    return jnp.mean(sequence_to_pool, axis=0)\n",
    "\n",
    "def sum_pooling(sequence_to_pool):\n",
    "    return jnp.sum(sequence_to_pool, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "Linear_encoder_parameter  = init_mlp_parameters([1,3,5])\n",
    "seconday_parameters = init_mlp_parameters([5,5,5])\n",
    "LRU = init_lru_parameters(5, 5)\n",
    "Linear_decoder_parameter = init_mlp_parameters([5,3,2])\n",
    "\n",
    "model_parameters = [Linear_encoder_parameter, LRU, seconday_parameters, Linear_decoder_parameter]\n",
    "\n",
    "def model_forward(input_sequence, parameters):\n",
    "    Linear_encoder_parameter,  LRU, seconday_parameters, Linear_decoder_parameter = parameters\n",
    "\n",
    "    x = forward_mlp(Linear_encoder_parameter, input_sequence)\n",
    "    x = forward_LRU(LRU, x)\n",
    "    x = forward_mlp(seconday_parameters, x)\n",
    "    x = max_pooling(x)\n",
    "    x = forward_mlp_linear_layer(Linear_decoder_parameter, x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def model_loss(input_sequence, target_sequence, parameters):\n",
    "    y = model_forward(input_sequence, parameters)\n",
    "\n",
    "    # Binary cross entropy loss\n",
    "    return -jnp.mean(target_sequence * jnp.log(y) + (1-target_sequence) * jnp.log(1-y))\n",
    "\n",
    "@jit\n",
    "def model_grad(input_sequence, target_sequence, parameters):\n",
    "    return grad(model_loss, argnums=2)(input_sequence, target_sequence, parameters)\n",
    "\n",
    "@jit\n",
    "def parameter_update(parameters, gradients, learning_rate = 0.01):\n",
    "    new_parameters = []\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[0], gradients[0]):\n",
    "        im.append((parameter[0] - learning_rate * gradient[0], parameter[1] - learning_rate * gradient[1]))\n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[1], gradients[1]):\n",
    "        im.append(parameter - learning_rate * gradient)    \n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[2], gradients[2]):\n",
    "        im.append((parameter[0] - learning_rate * gradient[0], parameter[1] - learning_rate * gradient[1]))\n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    im = []\n",
    "    for parameter, gradient in zip(parameters[3], gradients[3]):\n",
    "        im.append((parameter[0] - learning_rate * gradient[0], parameter[1] - learning_rate * gradient[1]))\n",
    "\n",
    "    new_parameters.append(im)\n",
    "\n",
    "    return new_parameters\n",
    "\n",
    "\n",
    "# Test batch model forward\n",
    "batch_model_forward = vmap(model_forward, in_axes=(0, None))\n",
    "\n",
    "@jit\n",
    "def accuracy(input_sequences, target_sequences, parameters):\n",
    "    y = batch_model_forward(input_sequences, parameters)\n",
    "    return jnp.mean(jnp.argmax(y, axis=1) == jnp.argmax(target_sequences, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "waveforms = pkl.load(open(\"waveforms.pkl\", \"rb\"))\n",
    "\n",
    "np.concat = np.concatenate\n",
    "\n",
    "sequences = np.concat((waveforms[\"waveform1\"], waveforms[\"waveform2\"]))\n",
    "labels = np.concat((np.zeros(500),np.ones(500)))\n",
    "\n",
    "# Shuffle the dataset\n",
    "perm = np.random.permutation(1000)\n",
    "sequences = sequences[perm]\n",
    "labels = labels[perm]\n",
    "labels = one_hot(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4000, 1)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(sequences[:2].reshape((2, len(sequences[0]), 1)).shape)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    print(batch_model_forward(sequences[:2].reshape((2, len(sequences[0]), 1)), model_parameters).shape)\n",
    "except:\n",
    "    print(\"Model forward failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 800/800 [100%] in 14.2s (56.32/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 1\n",
      "|████████████████████████████████████████| 800/800 [100%] in 7.0s (114.35/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 2\n",
      "|████████████████████████████████████████| 800/800 [100%] in 6.8s (117.57/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 3\n",
      "|████████████████████████████████████████| 800/800 [100%] in 7.0s (113.99/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 4\n",
      "|████████████████████████████████████████| 800/800 [100%] in 7.0s (115.09/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 5\n",
      "|████████████████████████████████████████| 800/800 [100%] in 7.6s (104.89/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 6\n",
      "|████████████████████████████████████████| 800/800 [100%] in 5.8s (136.79/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 7\n",
      "|████████████████████████████████████████| 800/800 [100%] in 8.1s (98.88/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 8\n",
      "|████████████████████████████████████████| 800/800 [100%] in 9.0s (88.60/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n",
      "Epoch 9\n",
      "|████████████████████████████████████████| 800/800 [100%] in 9.7s (82.05/s) \n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batchsize = 1\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "train_sequences = sequences[:800].reshape((800, len(sequences[0]), 1))\n",
    "train_labels = labels[:800]\n",
    "\n",
    "test_sequences = sequences[800:].reshape((200, len(sequences[0]), 1))\n",
    "test_labels = labels[800:]\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    with alive_bar(800) as bar:\n",
    "        for x, y in zip(train_sequences, train_labels):\n",
    "            gradients = model_grad(x, jnp.array([y]), model_parameters)\n",
    "            model_parameters = parameter_update(model_parameters, gradients)\n",
    "            bar()\n",
    "\n",
    "    train_acc.append(accuracy(train_sequences, train_labels, model_parameters))\n",
    "    test_acc.append(accuracy(test_sequences, test_labels, model_parameters))\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc[i]}\")\n",
    "    print(f\"Test Accuracy: {test_acc[i]}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.00297597, 0.997024  ], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sequences[0].reshape((len(sequences[0]), 1))\n",
    "y = jnp.array(labels[0])\n",
    "\n",
    "value = model_grad(x, y, model_parameters)\n",
    "\n",
    "model_forward(x, model_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
